apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: rayjob-hf-accelerate
  namespace: chirag-gpu-dev
spec:
  shutdownAfterJobFinishes: true
  ttlSecondsAfterFinished: 300
  entrypoint: python kuberay-examples/distributed/multinode/train.py
  runtimeEnvYAML: |
    pip:
      - --extra-index-url https://download.pytorch.org/whl/cu121
      - ray[data,train,tune]==2.40.0
      - accelerate==1.2.1
      - datasets==3.2.0
      - huggingface-hub==0.27.0
      - transformers==4.47.0
      - hf-transfer==0.1.8
      - torch==2.5.1+cu121
      - torchvision==0.20.0+cu121
    working_dir: "https://github.com/chiragjn/experiments/archive/master.zip"
    env_vars:
      NUM_WORKERS: "2"
  # rayClusterSpec specifies the RayCluster instance to be created by the RayJob controller.
  rayClusterSpec:
    rayVersion: '2.9.0'
    headGroupSpec:
      rayStartParams: {}
      # Pod template
      template:
        spec:
          volumes:
            - name: shared-volume
              persistentVolumeClaim:
                claimName: distributed-training-shared
            - emptyDir:
                medium: Memory
                sizeLimit: 3000M
              name: dshm
          containers:
            - name: ray-head
              image: tfy.jfrog.io/tfy-mirror/rayproject/ray:2.9.0
              volumeMounts:
                - mountPath: /mnt/shared
                  name: shared-volume
                  subPath: ''
                - mountPath: /dev/shm
                  name: dshm
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265 # Ray dashboard
                  name: dashboard
                - containerPort: 10001
                  name: client
              resources:
                limits:
                  cpu: "1"
                  memory: "4Gi"
                requests:
                  cpu: "1"
                  memory: "4Gi"
    workerGroupSpecs:
      - replicas: 2
        minReplicas: 2
        maxReplicas: 2
        groupName: small-group
        rayStartParams: {}
        # Pod template
        template:
          spec:
            volumes:
              - name: shared-volume
                persistentVolumeClaim:
                  claimName: distributed-training-shared
              - emptyDir:
                  medium: Memory
                  sizeLimit: 8000M
                name: dshm
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                        - key: karpenter.k8s.aws/instance-family
                          operator: In
                          values:
                            - g5
                        - key: karpenter.k8s.aws/instance-size
                          operator: In
                          values:
                            - xlarge
                            - 2xlarge
                            - 4xlarge
                            - 8xlarge
                        - key: karpenter.sh/capacity-type
                          operator: In
                          values:
                            - on-demand
            containers:
              - name: ray-worker
                image: tfy.jfrog.io/tfy-mirror/rayproject/ray:2.9.0
                volumeMounts:
                  - mountPath: /mnt/shared
                    name: shared-volume
                    subPath: ''
                  - mountPath: /dev/shm
                    name: dshm
                resources:
                  limits:
                    cpu: "3"
                    memory: "12Gi"
                    nvidia.com/gpu: 1
                  requests:
                    cpu: "3"
                    memory: "12Gi"
